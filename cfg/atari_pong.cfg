# Program
program_seed=0 # assign a program seed
program_auto_seed=true # true for assigning a random seed automatically
program_quiet=false # true for silencing the error message

# Actor
actor_num_simulation=18 # simulation number of MCTS
actor_mcts_puct_base=19652 # hyperparameter for puct_bias in the PUCT formula of MCTS, determining the level of exploration
actor_mcts_puct_init=1.25 # hyperparameter for puct_bias in the PUCT formula of MCTS
actor_mcts_reward_discount=0.997 # discount factor for calculating Q values
actor_mcts_value_rescale=true # true for games whose rewards are not bounded in [-1, 1], e.g., Atari games
actor_mcts_think_batch_size=1 # the MCTS selection batch size; only works when running console
actor_mcts_think_time_limit=0 # the MCTS time limit in seconds, 0 represents disabling time limit (only uses actor_num_simulation); only works when running console
actor_select_action_by_count=false # true for selecting the action by the maximum MCTS count; should not be true together with actor_select_action_by_softmax_count
actor_select_action_by_softmax_count=true # true for selecting the action by the propotion of MCTS count; should not be true together with actor_select_action_by_count
actor_select_action_softmax_temperature=1 # the softmax temperature when using actor_select_action_by_softmax_count
# true for decaying the temperature based on training iteration; set 1, 0.5, and 0.25 for 0%-50%, 50%-75%, and 75%-100% of total iterations, respectively
actor_select_action_softmax_temperature_decay=false
actor_use_random_rotation_features=true # true for randomly rotating input features; only supports in alphazero
actor_use_dirichlet_noise=false # true for adding dirchlet noise to the policy
actor_dirichlet_noise_alpha=0.25 # hyperparameter for dirchlet noise, usually (1 / sqrt(number of actions))
actor_dirichlet_noise_epsilon=0.25 # hyperparameter for dirchlet noise
actor_use_gumbel=true # true for enabling Gumbel Zero
actor_use_gumbel_noise=true # true for adding Gumbel noise to the policy
actor_gumbel_sample_size=18 # hyperparameter for Gumbel Zero; the number of sampled actions
actor_gumbel_sigma_visit_c=50 # hyperparameter for the monotonically increasing transformation sigma in Gumbel Zero
actor_gumbel_sigma_scale_c=0.1 # hyperparameter for the monotonically increasing transformation sigma in Gumbel Zero
actor_resign_threshold=-2 # the threshold determining when to resign in the actor

# Zero
zero_num_threads=4 # the number of threads that the zero server uses for zero training
zero_num_parallel_games=32 # the number of games to be run in parallel for zero training
zero_server_port=25000 # the port number to host the server; workers should connect to this port number
zero_training_directory= # the output directory name for storing training results
zero_num_games_per_iteration=250 # the nunmber of games to play in each iteration
zero_start_iteration=0 # the first iteration of training; usually 1 unless continuing with previous training
zero_end_iteration=300 # the last iteration of training
zero_replay_buffer=20 # hyperparameter for replay buffer; replay buffer stores (zero_replay_buffer x zero_num_games_per_iteration) games/sequences
zero_disable_resign_ratio=1 # the probability to keep playing when the winrate is below actor_resign_threshold
zero_actor_intermediate_sequence_length=200 # the max sequence length when running self-play; usually 0 (unlimited) for board games, 200 for atari games
zero_actor_ignored_command=reset_actors # the commands to ignore by the actor; format: command1 command2 ...
zero_server_accept_different_model_games=true # true for accepting self-play games generated by out-of-date model

# Learner
learner_use_per=true # true for enabling Prioritized Experience Replay
learner_per_alpha=1 # hyperparameter for PER that controlls the probability of sampling transition
learner_per_init_beta=0.4 # hyperparameter for PER that sets the initial beta value of linearly annealing
learner_per_beta_anneal=false # hyperparameter for PER that enables linearly anneal beta based on current training iteration
learner_training_step=200 # the number of training steps for updating the model in each iteration
learner_training_display_step=50 # the training step interval to display training information
learner_batch_size=512 # the batch size for training
learner_muzero_unrolling_step=5 # the number of steps to unroll for muzero training
learner_n_step_return=5 # the number of steps to calculate the n-step value; usually 0 for board games, 10 for atari games
learner_learning_rate=0.1 # hyperparameter for learning rate
learner_momentum=0.9 # hyperparameter for momentum
learner_weight_decay=0.0001 # hyperparameter for weight decay
learner_value_loss_scale=1 # hyperparameter for scaling of the value loss
learner_use_state_consistency=true # true for enabling state consistency
learner_num_thread=12 # the number of threads for training

# Network
nn_file_name= # the file name of model weights
nn_num_blocks=2 # hyperparameter for the model; the number of the residual blocks
nn_num_hidden_channels=64 # hyperparameter for the model; the size of the hidden channels in residual blocks
nn_num_value_hidden_channels=64 # hyperparameter for the model; the size of the hidden channels in the value network
nn_state_consistency_proj_hid=256 # hyperparameter for state consistency
nn_state_consistency_proj_out=256 # hyperparameter for state consistency
nn_state_consistency_pred_hid=64 # hyperparameter for state consistency
nn_state_consistency_pred_out=256 # hyperparameter for state consistency
nn_type_name=muzero # the type of training algorithm and network: alphazero/muzero

# Decoder
decoder_sgf_file_path=. # for decoder analysis
decoder_out_file_path=. # for decoder analysis
decoder_eval_sample_num=100 # for decoder analysis
decoder_output_at_inference=false # whether to forward decoder network at initial and recurrent inferences
decoder_loss_scale=25 # hyperparameter for scaling of the decoder loss
decoder_clip_grad_value=0.001 # the value to clip the gradient using clip_grad_value_; 0 to disable this clipping

# Environment
env_board_size=0 # the size of board
env_atari_rom_dir=/opt/atari57/ # the file path of the atari rom
# the atari game to play; supported 57 atari games:
#	alien amidar assault asterix asteroids atlantis bank_heist battle_zone beam_rider berzerk
#	bowling boxing breakout centipede chopper_command crazy_climber defender demon_attack double_dunk enduro
#	fishing_derby freeway frostbite gopher gravitar hero ice_hockey jamesbond kangaroo krull
#	kung_fu_master montezuma_revenge ms_pacman name_this_game phoenix pitfall pong private_eye qbert riverraid
#	road_runner robotank seaquest skiing solaris space_invaders star_gunner surround tennis time_pilot
#	tutankham up_n_down venture video_pinball wizard_of_wor yars_revenge zaxxon
env_atari_name=pong

